"""
æ¨¡å‹å¯¹æ¯”å·¥å…· - å¯¹æ¯”ä¸åŒç‰ˆæœ¬æ¨¡å‹çš„æ€§èƒ½
"""
import json
import argparse
from pathlib import Path
from tabulate import tabulate


def load_training_history(models_dir="hpf_platform/ml/models"):
    """åŠ è½½è®­ç»ƒå†å²"""
    history_file = Path(models_dir) / "training_history.json"
    
    if not history_file.exists():
        print(f"âŒ æœªæ‰¾åˆ°è®­ç»ƒå†å²æ–‡ä»¶: {history_file}")
        return []
    
    with open(history_file, 'r', encoding='utf-8') as f:
        history = json.load(f)
    
    return history


def display_all_runs(history):
    """æ˜¾ç¤ºæ‰€æœ‰è®­ç»ƒè½®æ¬¡"""
    if not history:
        print("ğŸ“Š æš‚æ— è®­ç»ƒè®°å½•")
        return
    
    print(f"\nğŸ“Š å…±æœ‰ {len(history)} è½®è®­ç»ƒè®°å½•\n")
    
    # å‡†å¤‡è¡¨æ ¼æ•°æ®
    table_data = []
    for i, run in enumerate(history, 1):
        table_data.append([
            i,
            run['timestamp'],
            run['model_type'],
            f"{run['f1_score']:.4f}",
            f"{run['precision']:.4f}",
            f"{run['recall']:.4f}",
            f"{run['accuracy']:.4f}",
            run['data_size']
        ])
    
    headers = ['#', 'æ—¶é—´', 'æ¨¡å‹', 'F1', 'Precision', 'Recall', 'Accuracy', 'æ•°æ®é‡']
    print(tabulate(table_data, headers=headers, tablefmt='grid'))
    
    # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
    best_run = max(history, key=lambda x: x['f1_score'])
    print(f"\nğŸ† å†å²æœ€ä½³:")
    print(f"   æ—¶é—´: {best_run['timestamp']}")
    print(f"   æ¨¡å‹: {best_run['model_type']}")
    print(f"   F1-Score: {best_run['f1_score']:.4f}")
    print(f"   æ–‡ä»¶: {best_run['model_path']}")


def compare_two_runs(history, run1_idx, run2_idx):
    """å¯¹æ¯”ä¸¤ä¸ªè®­ç»ƒè½®æ¬¡"""
    if run1_idx < 1 or run1_idx > len(history):
        print(f"âŒ æ— æ•ˆçš„è½®æ¬¡ç¼–å·: {run1_idx}")
        return
    
    if run2_idx < 1 or run2_idx > len(history):
        print(f"âŒ æ— æ•ˆçš„è½®æ¬¡ç¼–å·: {run2_idx}")
        return
    
    run1 = history[run1_idx - 1]
    run2 = history[run2_idx - 1]
    
    print(f"\nğŸ” å¯¹æ¯”åˆ†æ: ç¬¬{run1_idx}è½® vs ç¬¬{run2_idx}è½®\n")
    
    # å‡†å¤‡å¯¹æ¯”æ•°æ®
    metrics = ['f1_score', 'precision', 'recall', 'accuracy']
    table_data = []
    
    for metric in metrics:
        val1 = run1[metric]
        val2 = run2[metric]
        diff = val2 - val1
        diff_pct = (diff / val1 * 100) if val1 != 0 else 0
        
        diff_str = f"{diff:+.4f} ({diff_pct:+.2f}%)"
        if diff > 0:
            diff_str = f"ğŸŸ¢ {diff_str}"
        elif diff < 0:
            diff_str = f"ğŸ”´ {diff_str}"
        else:
            diff_str = "â– æ— å˜åŒ–"
        
        table_data.append([
            metric.replace('_', ' ').title(),
            f"{val1:.4f}",
            f"{val2:.4f}",
            diff_str
        ])
    
    headers = ['æŒ‡æ ‡', f'è½®æ¬¡ {run1_idx}', f'è½®æ¬¡ {run2_idx}', 'å˜åŒ–']
    print(tabulate(table_data, headers=headers, tablefmt='grid'))
    
    print(f"\nğŸ“… æ—¶é—´å¯¹æ¯”:")
    print(f"   è½®æ¬¡ {run1_idx}: {run1['timestamp']}")
    print(f"   è½®æ¬¡ {run2_idx}: {run2['timestamp']}")
   
    print(f"\nğŸ¤– æ¨¡å‹å¯¹æ¯”:")
    print(f"   è½®æ¬¡ {run1_idx}: {run1['model_type']}")
    print(f"   è½®æ¬¡ {run2_idx}: {run2['model_type']}")
    
    print(f"\nğŸ“Š æ•°æ®å¯¹æ¯”:")
    print(f"   è½®æ¬¡ {run1_idx}: {run1['data_size']} æ ·æœ¬")
    print(f"   è½®æ¬¡ {run2_idx}: {run2['data_size']} æ ·æœ¬")


def show_progress_trend(history):
    """æ˜¾ç¤ºæ€§èƒ½è¶‹åŠ¿"""
    if len(history) < 2:
        print("ğŸ“ˆ éœ€è¦è‡³å°‘2è½®è®­ç»ƒæ‰èƒ½æ˜¾ç¤ºè¶‹åŠ¿")
        return
    
    print("\nğŸ“ˆ F1-Score è¿›æ­¥è¶‹åŠ¿:\n")
    
    print("è½®æ¬¡  |  F1-Score  |  æ¨¡å‹ç±»å‹       |  è¶‹åŠ¿")
    print("-" * 60)
    
    for i, run in enumerate(history, 1):
        f1 = run['f1_score']
        model = run['model_type']
        
        if i == 1:
            trend = "  â€”  (åŸºçº¿)"
        else:
            prev_f1 = history[i-2]['f1_score']
            diff = f1 - prev_f1
            if diff > 0.001:
                trend = f"  â¬†ï¸  +{diff:.4f}"
            elif diff < -0.001:
                trend = f"  â¬‡ï¸  {diff:.4f}"
            else:
                trend = "  â¡ï¸  æŒå¹³"
        
        print(f"  {i:2d}  |  {f1:.4f}    |  {model:15s} | {trend}")
    
    # è®¡ç®—æ€»æå‡
    first_f1 = history[0]['f1_score']
    last_f1 = history[-1]['f1_score']
    total_improvement = last_f1 - first_f1
    total_pct = (total_improvement / first_f1 * 100) if first_f1 != 0 else 0
    
    print(f"\nâœ¨ æ€»ä½“æå‡: {total_improvement:+.4f} ({total_pct:+.2f}%)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¨¡å‹ç‰ˆæœ¬å¯¹æ¯”å·¥å…·")
    parser.add_argument("--models-dir", default="hpf_platform/ml/models", help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--compare", nargs=2, type=int, metavar=('RUN1', 'RUN2'), 
                       help="å¯¹æ¯”ä¸¤ä¸ªè®­ç»ƒè½®æ¬¡ï¼Œä¾‹å¦‚: --compare 1 3")
    parser.add_argument("--trend", action="store_true", help="æ˜¾ç¤ºæ€§èƒ½è¶‹åŠ¿")
    
    args = parser.parse_args()
    
    # åŠ è½½å†å²
    history = load_training_history(args.models_dir)
    
    if not history:
        exit(1)
    
    # æ ¹æ®å‚æ•°æ‰§è¡Œä¸åŒæ“ä½œ
    if args.compare:
        compare_two_runs(history, args.compare[0], args.compare[1])
    elif args.trend:
        show_progress_trend(history)
    else:
        # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰è½®æ¬¡
        display_all_runs(history)
        
        # å¦‚æœæœ‰å¤šè½®ï¼Œä¹Ÿæ˜¾ç¤ºè¶‹åŠ¿
        if len(history) > 1:
            show_progress_trend(history)
