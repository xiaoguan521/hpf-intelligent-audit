"""
Oracle åˆ° DuckDB å¢é‡åŒæ­¥ä¸»ç¨‹åº
ä½¿ç”¨ dlt å®ç°å¢é‡æ•°æ®åŠ è½½
æ”¯æŒå•çº¿ç¨‹å’Œå¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
"""
import dlt
from sqlalchemy import create_engine
from typing import Iterator, Dict, Any, List, Generator
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
import logging
import time
from pathlib import Path
from datetime import datetime
from decimal import Decimal
import oracledb
import sys
import os

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.pathï¼Œè§£å†³ä»å­ç›®å½•è¿è¡Œæ—¶çš„å¯¼å…¥é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Oracle åˆå§‹åŒ–ï¼ˆä½¿ç”¨æ™ºèƒ½é…ç½®ï¼‰
# ============================================================================

from hpf_platform.etl.config import OracleConfig

# æ™ºèƒ½åˆå§‹åŒ– Oracle å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ£€æµ‹ç‰ˆæœ¬å¹¶é€‰æ‹©æ¨¡å¼ï¼‰
oracle_mode = OracleConfig.init_oracle_client()
print(f"ğŸ”§ Oracle æ¨¡å¼: {oracle_mode.upper()}")
if OracleConfig.get_version() != "æœªçŸ¥":
    print(f"ğŸ“Š Oracle ç‰ˆæœ¬: {OracleConfig.get_version()}")


# ============================================================================
# Oracle åˆ° PyArrow ç±»å‹æ˜ å°„
# ============================================================================

def oracle_type_to_pyarrow(oracle_type: str, precision: int = None, scale: int = None):
    """
    å°† Oracle æ•°æ®ç±»å‹æ˜ å°„åˆ° PyArrow ç±»å‹
    
    Args:
        oracle_type: Oracle æ•°æ®ç±»å‹åç§°
        precision: ç²¾åº¦ï¼ˆé’ˆå¯¹ NUMBER ç±»å‹ï¼‰
        scale: å°æ•°ä½æ•°ï¼ˆé’ˆå¯¹ NUMBER ç±»å‹ï¼‰
        
    Returns:
        PyArrow æ•°æ®ç±»å‹
    """
    import pyarrow as pa
    
    oracle_type = oracle_type.upper()
    
    # æ•°å­—ç±»å‹
    if oracle_type == 'NUMBER':
        if scale == 0 or scale is None:
            # æ•´æ•°
            if precision is None or precision > 18:
                # å¤§æ•´æ•°ç”¨ decimal128(38, 0) ä¿å­˜ç²¾åº¦ï¼Œé¿å… float64 æº¢å‡ºæˆ–ç²¾åº¦ä¸¢å¤±
                return pa.decimal128(38, 0)
            elif precision <= 4:
                return pa.int16()
            elif precision <= 9:
                return pa.int32()
            elif precision <= 18:
                return pa.int64()
        else:
            # å°æ•°ç»Ÿä¸€ç”¨ float64 (é™¤ééœ€è¦é«˜ç²¾åº¦é‡‘é¢ï¼Œæš‚ä¿ç•™ float64 ä»¥å…¼å®¹æ—§é€»è¾‘)
            return pa.float64()
    
    if oracle_type in ('INTEGER', 'INT', 'SMALLINT'):
        return pa.int64()
    
    if oracle_type in ('FLOAT', 'BINARY_FLOAT'):
        return pa.float32()
    
    if oracle_type in ('DOUBLE PRECISION', 'BINARY_DOUBLE'):
        return pa.float64()
    
    # å­—ç¬¦ä¸²ç±»å‹
    if oracle_type in ('VARCHAR2', 'VARCHAR', 'NVARCHAR2', 'CHAR', 'NCHAR', 'CLOB', 'NCLOB', 'LONG'):
        return pa.string()
    
    # æ—¥æœŸæ—¶é—´ç±»å‹
    if oracle_type == 'DATE':
        return pa.timestamp('s')  # ç§’ç²¾åº¦
    
    if oracle_type.startswith('TIMESTAMP'):
        return pa.timestamp('us')  # å¾®ç§’ç²¾åº¦
    
    # äºŒè¿›åˆ¶ç±»å‹
    if oracle_type in ('BLOB', 'RAW', 'LONG RAW'):
        return pa.binary()
    
    # å…¶ä»–ç±»å‹ç»Ÿä¸€ç”¨ string
    return pa.string()


def get_table_schema_as_pyarrow(engine, table_name: str, schema: str = None):
    """
    ä» Oracle è·å–è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º PyArrow Schema
    
    Args:
        engine: SQLAlchemy engine
        table_name: è¡¨å
        schema: Oracle schema
        
    Returns:
        (pyarrow.Schema, List[str]) - PyArrow schema å’Œåˆ—ååˆ—è¡¨
    """
    import pyarrow as pa
    
    with engine.connect() as conn:
        raw_conn = conn.connection
        cursor = raw_conn.cursor()
        
        query = """
            SELECT COLUMN_NAME, DATA_TYPE, DATA_PRECISION, DATA_SCALE
            FROM ALL_TAB_COLUMNS 
            WHERE TABLE_NAME = UPPER(:tbl)
        """
        params = {'tbl': table_name}
        
        if schema:
            query += " AND OWNER = UPPER(:schema)"
            params['schema'] = schema
        
        query += " ORDER BY COLUMN_ID"
        
        cursor.execute(query, params)
        columns_info = cursor.fetchall()
        cursor.close()
        
        if not columns_info:
            return None, []
        
        fields = []
        column_names = []
        
        for col_name, data_type, precision, scale in columns_info:
            pa_type = oracle_type_to_pyarrow(data_type, precision, scale)
            # æ‰€æœ‰å­—æ®µéƒ½å…è®¸ null
            fields.append(pa.field(col_name, pa_type, nullable=True))
            column_names.append(col_name)
        
        return pa.schema(fields), column_names


# ============================================================================
# å¹¶è¡Œè¯»å–å™¨ï¼ˆç”¨äºå¤šçº¿ç¨‹æ¨¡å¼ï¼‰
# ============================================================================

class OracleParallelReader:
    """Oracle å¹¶è¡Œè¯»å–å™¨ - å¤šçº¿ç¨‹åˆ†ç‰‡è¯»å–"""
    
    def __init__(
        self,
        connection_string: str,
        table_name: str,
        schema: str = None,
        primary_key: str = "ID",
        num_workers: int = 4,
        batch_size: int = 50000
    ):
        self.connection_string = connection_string
        self.table_name = table_name
        self.schema = schema
        self.primary_key = primary_key
        self.num_workers = num_workers
        self.batch_size = batch_size
        self.full_table = f"{schema}.{table_name}" if schema else table_name
        self.stats = {'read': 0, 'start_time': None}
    
    def _get_engine(self):
        """åˆ›å»ºæ•°æ®åº“å¼•æ“"""
        return create_engine(
            self.connection_string,
            pool_size=self.num_workers + 2,
            max_overflow=self.num_workers,
            pool_pre_ping=True,
            pool_recycle=3600,
            echo=False
        )
    
    def get_columns(self, engine) -> List[str]:
        """è·å–è¡¨çš„åˆ—å"""
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            
            query = "SELECT COLUMN_NAME FROM ALL_TAB_COLUMNS WHERE TABLE_NAME = UPPER(:tbl)"
            params = {'tbl': self.table_name}
            
            if self.schema:
                query += " AND OWNER = UPPER(:schema)"
                params['schema'] = self.schema
            
            query += " ORDER BY COLUMN_ID"
            cursor.execute(query, params)
            columns = [row[0] for row in cursor.fetchall()]
            cursor.close()
            
            return columns
    
    def get_id_range(self, engine, last_value: int = None) -> tuple:
        """è·å– ID èŒƒå›´"""
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            
            if last_value:
                query = f"SELECT :last_val, MAX({self.primary_key}), COUNT(*) FROM {self.full_table} WHERE {self.primary_key} > :last_val"
                cursor.execute(query, {'last_val': last_value})
            else:
                query = f"SELECT MIN({self.primary_key}), MAX({self.primary_key}), COUNT(*) FROM {self.full_table}"
                cursor.execute(query)
            
            result = cursor.fetchone()
            cursor.close()
            
            min_id = (result[0] - 1) if result[0] else 0
            max_id = result[1] or 0
            count = result[2] or 0
            
            return min_id, max_id, count
    
    def calculate_chunks(self, min_id: int, max_id: int, total_count: int) -> List[tuple]:
        """
        ä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰åˆ†ç‰‡è¾¹ç•Œç‚¹
        ä½¿ç”¨ ROW_NUMBER + MOD ç­›é€‰è¾¹ç•Œè¡Œ
        """
        if total_count == 0:
            return [(0, min_id, max_id + 1)]
        
        num_chunks = min(self.num_workers * 2, total_count)
        
        if num_chunks <= 1:
            return [(0, min_id, max_id + 1)]
        
        rows_per_chunk = total_count // num_chunks
        
        engine = self._get_engine()
        try:
            with engine.connect() as conn:
                raw_conn = conn.connection
                cursor = raw_conn.cursor()
                
                # ç›´æ¥åœ¨ SQL ä¸­ä½¿ç”¨æ•°å€¼ï¼Œé¿å…ç»‘å®šå˜é‡é—®é¢˜
                query = f"""
                    SELECT pk_val FROM (
                        SELECT {self.primary_key} as pk_val,
                               ROW_NUMBER() OVER (ORDER BY {self.primary_key}) as rn
                        FROM {self.full_table}
                    )
                    WHERE rn = 1 
                       OR MOD(rn, {rows_per_chunk}) = 0
                       OR rn = {total_count}
                    ORDER BY pk_val
                """
                cursor.execute(query)
                boundary_ids = [row[0] for row in cursor.fetchall()]
                cursor.close()
                
                # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºå®é™…çš„è¾¹ç•ŒID
                print(f"  â”œâ”€ è¾¹ç•ŒID: {[f'{x:,}' for x in boundary_ids[:5]]}...{[f'{x:,}' for x in boundary_ids[-2:]]}")
                
                if len(boundary_ids) < 2:
                    return [(0, min_id, max_id + 1)]
                
                # æ„å»ºåˆ†ç‰‡ï¼šç¬¬ä¸€ä¸ªåˆ†ç‰‡ä»ç¬¬ä¸€ä¸ªè¾¹ç•ŒID-1å¼€å§‹ï¼ˆé¿å…å¤§é‡ç©ºæ´æ‰«æï¼‰
                chunks = []
                for i in range(len(boundary_ids) - 1):
                    # æ‰€æœ‰åˆ†ç‰‡éƒ½ä½¿ç”¨å®é™…çš„è¾¹ç•ŒID
                    start = boundary_ids[i] - 1  # ID > startï¼Œæ‰€ä»¥å‡1ç¡®ä¿åŒ…å«è¾¹ç•Œå€¼
                    end = boundary_ids[i + 1]
                    chunks.append((i, start, end))
                
                return chunks
        finally:
            engine.dispose()
    
    def read_chunk(self, engine, chunk: tuple, columns: List[str]) -> List[Dict]:
        """è¯»å–å•ä¸ªåˆ†ç‰‡ - ä½¿ç”¨ç‹¬ç«‹è¿æ¥é¿å…å¹¶å‘é—®é¢˜"""
        chunk_id, start_id, end_id = chunk
        
        print(f"\n  [åˆ†ç‰‡ {chunk_id}] å¼€å§‹è¯»å–: ID {start_id:,} ~ {end_id:,}")
        chunk_start = time.time()
        
        from sqlalchemy import create_engine as ce
        local_engine = ce(
            self.connection_string,
            pool_size=1,
            max_overflow=0,
            pool_pre_ping=True,
            echo=False
        )
        
        try:
            with local_engine.connect() as conn:
                raw_conn = conn.connection
                cursor = raw_conn.cursor()
                cursor.arraysize = min(self.batch_size, 10000)
                cursor.prefetchrows = cursor.arraysize
                
                columns_str = ", ".join(columns)
                query = f"""
                    SELECT {columns_str} FROM {self.full_table}
                    WHERE {self.primary_key} > :start_id AND {self.primary_key} <= :end_id
                    ORDER BY {self.primary_key}
                """
                
                cursor.execute(query, {'start_id': start_id, 'end_id': end_id})
                rows = cursor.fetchall()
                cursor.close()
                
                chunk_elapsed = time.time() - chunk_start
                print(f"  [åˆ†ç‰‡ {chunk_id}] å®Œæˆ: {len(rows):,} è¡Œ, è€—æ—¶ {chunk_elapsed:.1f}ç§’")
                
                return [dict(zip(columns, row)) for row in rows]
        finally:
            local_engine.dispose()
    
    def get_partitions(self, engine) -> List[str]:
        """è·å–è¡¨çš„åˆ†åŒºåˆ—è¡¨"""
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            
            # è·å–è¡¨çš„æ‰€æœ‰åˆ†åŒºå
            table_owner = self.schema.upper() if self.schema else None
            table_name = self.table_name.upper()
            
            query = """
                SELECT PARTITION_NAME 
                FROM ALL_TAB_PARTITIONS 
                WHERE TABLE_NAME = :table_name
            """
            params = {'table_name': table_name}
            
            if table_owner:
                query += " AND TABLE_OWNER = :table_owner"
                params['table_owner'] = table_owner
            
            query += " ORDER BY PARTITION_POSITION"
            
            cursor.execute(query, params)
            partitions = [row[0] for row in cursor.fetchall()]
            cursor.close()
            
            return partitions
    
    def read_partition(self, partition_name: str, columns: List[str]) -> List[Dict]:
        """è¯»å–å•ä¸ªåˆ†åŒºçš„æ‰€æœ‰æ•°æ®ï¼ˆåˆ†æ‰¹è¯»å–ï¼‰"""
        print(f"\n  [åˆ†åŒº {partition_name}] å¼€å§‹è¯»å–...")
        partition_start = time.time()
        
        from sqlalchemy import create_engine as ce
        local_engine = ce(
            self.connection_string,
            pool_size=1,
            max_overflow=0,
            pool_pre_ping=True,
            echo=False
        )
        
        all_rows = []
        try:
            with local_engine.connect() as conn:
                raw_conn = conn.connection
                cursor = raw_conn.cursor()
                cursor.arraysize = 10000
                cursor.prefetchrows = 10000
                
                columns_str = ", ".join(columns)
                query = f"""
                    SELECT {columns_str} 
                    FROM {self.full_table} PARTITION ({partition_name})
                    ORDER BY {self.primary_key}
                """
                
                cursor.execute(query)
                
                # åˆ†æ‰¹è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šæ•°æ®åˆ°å†…å­˜
                batch_size = 50000
                batch_count = 0
                while True:
                    rows = cursor.fetchmany(batch_size)
                    if not rows:
                        break
                    
                    batch_count += 1
                    all_rows.extend([dict(zip(columns, row)) for row in rows])
                    
                    # æ¯éš”ä¸€æ‰¹æ‰“å°è¿›åº¦
                    elapsed = time.time() - partition_start
                    print(f"  [åˆ†åŒº {partition_name}] è¯»å–ä¸­: {len(all_rows):,} è¡Œ | æ‰¹æ¬¡ #{batch_count} | è€—æ—¶ {elapsed:.1f}ç§’", end='\r')
                
                cursor.close()
                
                partition_elapsed = time.time() - partition_start
                print(f"  [åˆ†åŒº {partition_name}] å®Œæˆ: {len(all_rows):,} è¡Œ, è€—æ—¶ {partition_elapsed:.1f}ç§’    ")
                
                return all_rows
        finally:
            local_engine.dispose()
    
    def parallel_read_by_partition(self) -> Generator[Dict, None, None]:
        """æŒ‰åˆ†åŒºä¸²è¡Œè¯»å–ï¼ˆè¾¹è¯»è¾¹å†™ï¼ŒèŠ‚çœå†…å­˜ï¼‰"""
        self.stats['start_time'] = time.time()
        self.stats['read'] = 0
        
        engine = self._get_engine()
        
        try:
            columns = self.get_columns(engine)
            partitions = self.get_partitions(engine)
            
            print(f"  â”œâ”€ è¡¨: {self.full_table}")
            print(f"  â”œâ”€ åˆ—æ•°: {len(columns)}")
            print(f"  â”œâ”€ åˆ†åŒºæ•°: {len(partitions)}")
            print(f"  â”œâ”€ è¯»å–æ¨¡å¼: æŒ‰åˆ†åŒºä¸²è¡Œï¼ˆè¾¹è¯»è¾¹å†™ï¼‰")
            
            if not partitions:
                print(f"  â””â”€ éåˆ†åŒºè¡¨ï¼Œé€€å›åˆ° ID åˆ†ç‰‡æ¨¡å¼")
                yield from self.parallel_read(None)
                return
            
            print(f"  â”œâ”€ åˆ†åŒºåˆ—è¡¨: {partitions[:5]}...{partitions[-2:] if len(partitions) > 5 else ''}")
            
            # ä¸²è¡Œå¤„ç†æ¯ä¸ªåˆ†åŒºï¼Œè¾¹è¯»è¾¹å†™
            for idx, partition_name in enumerate(partitions):
                partition_start = time.time()
                partition_rows = 0
                
                # ç›´æ¥ä½¿ç”¨ engine è¯»å–ï¼Œä¸åˆ›å»ºæ–°è¿æ¥
                with engine.connect() as conn:
                    raw_conn = conn.connection
                    cursor = raw_conn.cursor()
                    cursor.arraysize = 10000
                    cursor.prefetchrows = 10000
                    
                    columns_str = ", ".join(columns)
                    query = f"""
                        SELECT {columns_str} 
                        FROM {self.full_table} PARTITION ({partition_name})
                        ORDER BY {self.primary_key}
                    """
                    
                    cursor.execute(query)
                    
                    # åˆ†æ‰¹è¯»å–å¹¶ç«‹å³ yield
                    batch_count = 0
                    while True:
                        rows = cursor.fetchmany(50000)
                        if not rows:
                            break
                        
                        batch_count += 1
                        for row in rows:
                            yield dict(zip(columns, row))
                            self.stats['read'] += 1
                            partition_rows += 1
                        
                        # å®æ—¶è¿›åº¦
                        elapsed = time.time() - self.stats['start_time']
                        speed = self.stats['read'] / elapsed if elapsed > 0 else 0
                        print(f"  â”œâ”€ [{partition_name}] è¯»å–ä¸­: {partition_rows:,} è¡Œ | æ€»è®¡: {self.stats['read']:,} | é€Ÿåº¦: {speed:,.0f} è¡Œ/ç§’", end='\r')
                    
                    cursor.close()
                
                partition_elapsed = time.time() - partition_start
                print(f"  â”œâ”€ [{partition_name}] å®Œæˆ: {partition_rows:,} è¡Œ, è€—æ—¶ {partition_elapsed:.1f}ç§’ ({idx+1}/{len(partitions)})    ")
            
            elapsed = time.time() - self.stats['start_time']
            speed = self.stats['read'] / elapsed if elapsed > 0 else 0
            print(f"\n  â””â”€ è¯»å–å®Œæˆ: {self.stats['read']:,} è¡Œ | è€—æ—¶: {elapsed:.1f}ç§’ | å¹³å‡: {speed:,.0f} è¡Œ/ç§’")
        
        finally:
            engine.dispose()
    
    def parallel_read(self, last_value: int = None) -> Generator[Dict, None, None]:
        """å¹¶è¡Œè¯»å–æ‰€æœ‰æ•°æ®"""
        self.stats['start_time'] = time.time()
        self.stats['read'] = 0
        
        engine = self._get_engine()
        
        try:
            columns = self.get_columns(engine)
            min_id, max_id, total_count = self.get_id_range(engine, last_value)
            
            print(f"  â”œâ”€ è¡¨: {self.full_table}")
            print(f"  â”œâ”€ åˆ—æ•°: {len(columns)}")
            print(f"  â”œâ”€ ID èŒƒå›´: {min_id:,} ~ {max_id:,}")
            print(f"  â”œâ”€ é¢„è®¡è¡Œæ•°: {total_count:,}")
            print(f"  â”œâ”€ å¹¶è¡Œçº¿ç¨‹: {self.num_workers}")
            
            if total_count == 0:
                print(f"  â””â”€ æ— æ–°æ•°æ®")
                return
            
            print(f"  â”œâ”€ æ­£åœ¨è®¡ç®—åˆ†ç‰‡è¾¹ç•Œï¼ˆé‡‡æ ·æ–¹å¼ï¼‰...")
            chunks = self.calculate_chunks(min_id, max_id, total_count)
            print(f"  â”œâ”€ åˆ†ç‰‡æ•°é‡: {len(chunks)}")
            
            # é˜Ÿåˆ—å­˜å‚¨ç»“æœï¼Œä¿è¯é¡ºåº
            result_queue = Queue()
            completed_chunks = {}
            next_chunk_to_yield = 0
            
            def reader_callback(future, chunk_id):
                try:
                    data = future.result()
                    result_queue.put((chunk_id, data))
                except Exception as e:
                    logger.error(f"åˆ†ç‰‡ {chunk_id} è¯»å–å¤±è´¥: {e}")
                    result_queue.put((chunk_id, []))
            
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                futures = []
                for chunk in chunks:
                    future = executor.submit(self.read_chunk, engine, chunk, columns)
                    future.add_done_callback(lambda f, cid=chunk[0]: reader_callback(f, cid))
                    futures.append(future)
                
                chunks_received = 0
                while chunks_received < len(chunks):
                    try:
                        # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰ï¼Œé€‚åº”å¤§èŒƒå›´åˆ†ç‰‡
                        chunk_id, data = result_queue.get(timeout=600)
                        completed_chunks[chunk_id] = data
                        chunks_received += 1
                        
                        while next_chunk_to_yield in completed_chunks:
                            chunk_data = completed_chunks.pop(next_chunk_to_yield)
                            for row in chunk_data:
                                yield row
                                self.stats['read'] += 1
                            
                            next_chunk_to_yield += 1
                            
                            elapsed = time.time() - self.stats['start_time']
                            speed = self.stats['read'] / elapsed if elapsed > 0 else 0
                            print(f"  â”œâ”€ è¯»å–: {self.stats['read']:,} è¡Œ | é€Ÿåº¦: {speed:,.0f} è¡Œ/ç§’ | åˆ†ç‰‡: {next_chunk_to_yield}/{len(chunks)}", end='\r')
                    
                    except Empty:
                        # ä¸æ˜¯é”™è¯¯ï¼Œåªæ˜¯è¿˜åœ¨ç­‰å¾…
                        elapsed = time.time() - self.stats['start_time']
                        print(f"  â”œâ”€ ç­‰å¾…åˆ†ç‰‡å®Œæˆ... å·²ç­‰å¾… {elapsed:.0f} ç§’", end='\r')
            
            elapsed = time.time() - self.stats['start_time']
            speed = self.stats['read'] / elapsed if elapsed > 0 else 0
            print(f"\n  â””â”€ è¯»å–å®Œæˆ: {self.stats['read']:,} è¡Œ | è€—æ—¶: {elapsed:.1f}ç§’ | å¹³å‡: {speed:,.0f} è¡Œ/ç§’")
        
        finally:
            engine.dispose()


# ============================================================================
# dlt Resource å‡½æ•°
# ============================================================================

def oracle_table_resource(
    connection_string: str,
    table_name: str,
    schema: str = None,
    incremental_column: str = None,
    batch_size: int = 50000,
    stats: dict = None,
    primary_key: str = "ID",
    parallel: bool = False,
    num_workers: int = 4
) -> Iterator[Dict[str, Any]]:
    """
    ä» Oracle è¡¨ä¸­è¯»å–æ•°æ®çš„ dlt resource
    
    Args:
        connection_string: Oracle æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        table_name: è¦åŒæ­¥çš„è¡¨å
        schema: æ•°æ®åº“ schema (å¯é€‰)
        incremental_column: ç”¨äºå¢é‡åŠ è½½çš„åˆ—å
        batch_size: æ¯æ‰¹è¯»å–çš„è¡Œæ•°
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        primary_key: ä¸»é”®åˆ—ï¼ˆç”¨äºåˆ†é¡µï¼‰
        parallel: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
        num_workers: å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆä»… parallel=True æ—¶ç”Ÿæ•ˆï¼‰
    """
    full_table_name = f"{schema}.{table_name}" if schema else table_name
    
    # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
    if stats:
        stats[table_name] = {
            'rows': 0,
            'mode': 'å…¨é‡',
            'last_value': None
        }
    
    # é¢„å…ˆè·å–è¡¨çš„ PyArrow Schemaï¼ˆç»Ÿä¸€ Fast å’Œ Standard æ¨¡å¼ï¼‰
    engine = create_engine(
        connection_string,
        pool_size=1,
        max_overflow=0,
        pool_pre_ping=True,
        echo=False
    )
    pyarrow_schema, _ = get_table_schema_as_pyarrow(engine, table_name, schema)
    engine.dispose()
    
    # ä» Schema ä¸­æå– range_column çš„ç±»å‹
    range_column = incremental_column or primary_key
    range_col_type = None
    if pyarrow_schema:
        import pyarrow as pa
        for field in pyarrow_schema:
            if field.name == range_column:
                range_col_type = field.type
                break
    
    # è·å–å¢é‡çŠ¶æ€
    last_value = None
    if incremental_column:
        last_value = dlt.current.resource_state().get('last_value')
        if last_value:
            if stats:
                stats[table_name]['mode'] = 'å¢é‡'
                stats[table_name]['last_value'] = str(last_value)
            print(f"  â”œâ”€ å¢é‡æ¨¡å¼: ä» {incremental_column} > {last_value} å¼€å§‹")
    
    # é€‰æ‹©è¯»å–æ¨¡å¼
    if parallel:
        # å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
        print(f"  â”œâ”€ è¯»å–æ¨¡å¼: å¤šçº¿ç¨‹å¹¶è¡Œ ({num_workers} workers)")
        yield from _parallel_read(
            connection_string, table_name, schema, primary_key,
            num_workers, batch_size, incremental_column, last_value, stats
        )
    else:
        # å•çº¿ç¨‹é¡ºåºæ¨¡å¼
        print(f"  â”œâ”€ è¯»å–æ¨¡å¼: å•çº¿ç¨‹é¡ºåº")
        yield from _sequential_read(
            connection_string, table_name, schema, primary_key,
            batch_size, incremental_column, last_value, stats, range_col_type
        )


def _parallel_read(
    connection_string: str,
    table_name: str,
    schema: str,
    primary_key: str,
    num_workers: int,
    batch_size: int,
    incremental_column: str,
    last_value: int,
    stats: dict
) -> Generator[Dict, None, None]:
    """å¤šçº¿ç¨‹å¹¶è¡Œè¯»å–"""
    reader = OracleParallelReader(
        connection_string=connection_string,
        table_name=table_name,
        schema=schema,
        primary_key=primary_key,
        num_workers=num_workers,
        batch_size=batch_size
    )
    
    max_value = last_value
    row_count = 0
    
    for row in reader.parallel_read(last_value):
        if incremental_column and incremental_column in row:
            row_value = row[incremental_column]
            if max_value is None or row_value > max_value:
                max_value = row_value
        
        yield row
        row_count += 1
    
    # ä¿å­˜å¢é‡çŠ¶æ€
    if incremental_column and max_value is not None:
        dlt.current.resource_state()['last_value'] = max_value
    
    if stats:
        stats[table_name]['rows'] = row_count


def _sequential_read(
    connection_string: str,
    table_name: str,
    schema: str,
    primary_key: str,
    batch_size: int,
    incremental_column: str,
    last_value: int,
    stats: dict,
    range_col_type = None  # PyArrow ç±»å‹
) -> Generator[Dict, None, None]:
    """å•çº¿ç¨‹é¡ºåºè¯»å–ï¼ˆä½¿ç”¨ ID èŒƒå›´åˆ†é¡µï¼‰"""
    print(f"DEBUG: _sequential_read called with table_name='{table_name}', schema='{schema}'")
    # å¦‚æœè¡¨åå·²ç»åŒ…å« . (ä¾‹å¦‚ SCHEMA.TABLE)ï¼Œåˆ™ä¸å†æ‹¼æ¥ schema
    if "." in table_name:
        full_table_name = table_name
        # å°è¯•ä» table_name ä¸­æå–çœŸå®è¡¨åç”¨äºå…ƒæ•°æ®æŸ¥è¯¢
        real_table_name = table_name.split(".")[-1]
    else:
        full_table_name = f"{schema}.{table_name}" if schema else table_name
        real_table_name = table_name
    range_column = incremental_column or primary_key
    
    engine = create_engine(
        connection_string,
        pool_size=5,
        max_overflow=10,
        pool_pre_ping=True,
        pool_recycle=3600,
        echo=False
    )
    
    try:
        with engine.connect() as conn:
            raw_conn = conn.connection
            
            # è·å–åˆ—å
            columns_query = f"""
                SELECT COLUMN_NAME FROM ALL_TAB_COLUMNS 
                WHERE TABLE_NAME = UPPER(:tbl) 
                {"AND OWNER = UPPER(:schema)" if schema else ""}
                ORDER BY COLUMN_ID
            """
            cursor = raw_conn.cursor()
            if schema:
                cursor.execute(columns_query, {'tbl': real_table_name, 'schema': schema})
            else:
                cursor.execute(columns_query, {'tbl': real_table_name})
            columns = [row[0] for row in cursor.fetchall()]
            cursor.close()
            
            if not columns:
                print(f"  âš ï¸ æ— æ³•è·å–è¡¨ {real_table_name} çš„åˆ—ä¿¡æ¯ï¼Œå›é€€åˆ° SELECT *")
                columns_str = "*"
            else:
                columns_str = ", ".join(columns)
            
            # è·å–èµ·å§‹å€¼ï¼ˆä½¿ç”¨ä¼ å…¥çš„ PyArrow ç±»å‹åˆ¤æ–­ï¼‰
            if last_value is None:
                import pyarrow as pa
                # åˆ¤æ–­æ˜¯å¦ä¸º DATE ç±»å‹
                is_date_type = range_col_type and pa.types.is_timestamp(range_col_type)
                
                if is_date_type:
                    last_value = "DATE '1900-01-01'"  # SQL å­—é¢é‡
                else:
                    min_query = f"SELECT MIN({range_column}) FROM {full_table_name}"
                    cursor = raw_conn.cursor()
                    cursor.execute(min_query)
                    min_result = cursor.fetchone()
                    cursor.close()
                    last_value = (min_result[0] - 1) if min_result and min_result[0] else 0
            
            print(f"  â”œâ”€ è¡¨: {full_table_name}")
            print(f"  â”œâ”€ åˆ†é¡µåˆ—: {range_column}, èµ·å§‹å€¼: {last_value}")
            print(f"  â”œâ”€ æ‰¹é‡å¤§å°: {batch_size:,}")
            
            # ä¿å­˜åˆå§‹ last_valueï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºå¢é‡æ¢å¤
            initial_value = last_value
            
            row_count = 0
            batch_num = 0
            start_time = time.time()
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„å¢é‡æ¢å¤ï¼ˆä» dlt state æ¢å¤çš„å€¼ï¼‰
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡åŒæ­¥ï¼ˆlast_value åˆšä»å‡½æ•°å‚æ•°ä¼ å…¥çš„ None è®¡ç®—è€Œæ¥ï¼‰ï¼Œä½¿ç”¨ >= åŒ…å«è¾¹ç•Œ
            is_first_sync = (last_value == initial_value)
            
            
            while True:
                batch_num += 1
                
                # å¦‚æœ last_value æ˜¯ SQL å­—é¢é‡ï¼ˆDATEï¼‰ï¼Œç›´æ¥æ‹¼æ¥è€Œä¸æ˜¯ç”¨ç»‘å®šå˜é‡
                if isinstance(last_value, str) and last_value.startswith("DATE "):
                    # å¯¹äºé¦–æ¬¡åŒæ­¥ï¼Œä½¿ç”¨ >= ä»¥åŒ…å«è¾¹ç•Œå€¼å’Œæ‰€æœ‰é NULL æ•°æ®
                    operator = ">=" if is_first_sync else ">"
                    range_query = f"""
                        SELECT {columns_str} FROM (
                            SELECT {columns_str} FROM {full_table_name}
                            WHERE {range_column} {operator} {last_value}
                            ORDER BY {range_column}
                        ) WHERE ROWNUM <= :batch_size
                    """
                    bind_vars = {'batch_size': batch_size}
                else:
                    # å¯¹äºé¦–æ¬¡åŒæ­¥ï¼Œä½¿ç”¨ >= ä»¥åŒ…å«è¾¹ç•Œå€¼
                    operator = ">=" if is_first_sync else ">"
                    range_query = f"""
                        SELECT {columns_str} FROM (
                            SELECT {columns_str} FROM {full_table_name}
                            WHERE {range_column} {operator} :last_val
                            ORDER BY {range_column}
                        ) WHERE ROWNUM <= :batch_size
                    """
                    bind_vars = {'last_val': last_value, 'batch_size': batch_size}
                
                
                cursor = raw_conn.cursor()
                cursor.arraysize = min(batch_size, 10000)
                cursor.prefetchrows = cursor.arraysize
                
                cursor.execute(range_query, bind_vars)
                
                
                col_names = [desc[0] for desc in cursor.description]
                rows = cursor.fetchall()
                cursor.close()
                
                if not rows:
                    break
                
                range_col_idx = col_names.index(range_column) if range_column in col_names else 0
                last_value = rows[-1][range_col_idx]
                
                batch_data = [dict(zip(col_names, row)) for row in rows]
                
                if incremental_column:
                    dlt.current.resource_state()['last_value'] = last_value
                
                yield from batch_data
                
                row_count += len(batch_data)
                
                if stats:
                    stats[table_name]['rows'] = row_count
                    elapsed = time.time() - start_time
                    speed = row_count / elapsed if elapsed > 0 else 0
                    print(f"  â”œâ”€ è¿›åº¦: {row_count:,} è¡Œ | é€Ÿåº¦: {speed:,.0f} è¡Œ/ç§’ | æ‰¹æ¬¡ #{batch_num}", end='\r')
                
                if len(rows) < batch_size:
                    break
            
            elapsed = time.time() - start_time
            speed = row_count / elapsed if elapsed > 0 else 0
            if stats:
                stats[table_name]['rows'] = row_count
            print(f"\n  â””â”€ å®Œæˆ: {row_count:,} è¡Œ | æ€»è€—æ—¶: {elapsed:.1f}ç§’ | å¹³å‡: {speed:,.0f} è¡Œ/ç§’")
    
    except Exception as e:
        # ä¼˜åŒ–é”™è¯¯è¾“å‡ºï¼Œé¿å…æ»¡å± Tracebackï¼Œé™¤éæ˜¯è°ƒè¯•æ¨¡å¼
        error_msg = str(e)
        if "ORA-" in error_msg:
            print(f"\n  âŒ Oracle é”™è¯¯: {error_msg.split('Help:')[0].strip()}")
            if "ORA-00933" in error_msg:
                print(f"     æç¤º: SQL è¯­æ³•é”™è¯¯ï¼Œå½“å‰æŸ¥è¯¢è¡¨å: {full_table_name}")
        else:
            print(f"\n  âŒ åŒæ­¥å‡ºé”™: {error_msg}")
            import traceback
            # traceback.print_exc() # ç”¨æˆ·è¦æ±‚å‡å°‘æ··ä¹±è¾“å‡º
        
        raise Exception(error_msg)
    
    finally:
        engine.dispose()


# ============================================================================
# dlt Source å‡½æ•°
# ============================================================================

@dlt.source
def oracle_source(
    connection_string: str,
    tables: list[dict],
    stats: dict = None,
    parallel: bool = False,
    num_workers: int = 4
):
    """
    Oracle æ•°æ®æº
    
    Args:
        connection_string: Oracle è¿æ¥å­—ç¬¦ä¸²
        tables: è¦åŒæ­¥çš„è¡¨é…ç½®åˆ—è¡¨
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        parallel: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
        num_workers: å¹¶è¡Œçº¿ç¨‹æ•°
    """
    for table_config in tables:
        table_name = table_config["name"]
        incremental_column = table_config.get("incremental_column")
        schema = table_config.get("schema")
        batch_size = table_config.get("batch_size", 50000)
        primary_key = table_config.get("primary_key", "ID")
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        if stats is not None:
            stats[table_name] = {
                'rows': 0,
                'mode': 'æœªçŸ¥',
                'last_value': None
            }
        
        # 1. è·å–è¡¨ç»“æ„å¹¶è½¬æ¢ä¸º dlt columns
        dlt_columns = {}
        try:
            # åˆ›å»ºä¸´æ—¶å¼•æ“è·å–å…ƒæ•°æ®
            engine = create_engine(
                connection_string,
                pool_size=1, max_overflow=0,
                pool_pre_ping=True, echo=False
            )
            pa_schema, _ = get_table_schema_as_pyarrow(engine, table_name, schema)
            engine.dispose()
            
            if pa_schema:
                import pyarrow as pa
                for field in pa_schema:
                    dlt_type = "text"
                    t = field.type
                    if pa.types.is_string(t): dlt_type = "text"
                    elif pa.types.is_integer(t): dlt_type = "bigint"
                    elif pa.types.is_floating(t): dlt_type = "double"
                    elif pa.types.is_decimal(t): dlt_type = "decimal"
                    elif pa.types.is_timestamp(t): dlt_type = "timestamp"
                    elif pa.types.is_date(t): dlt_type = "date"
                    elif pa.types.is_binary(t): dlt_type = "binary"
                    elif pa.types.is_boolean(t): dlt_type = "bool"
                    dlt_columns[field.name] = {"name": field.name, "data_type": dlt_type, "nullable": field.nullable}
        except Exception as e:
            print(f"  âš ï¸ Schema é¢„è·å–å¤±è´¥: {e}")
        
        yield dlt.resource(
            oracle_table_resource(
                connection_string=connection_string,
                table_name=table_name,
                schema=schema,
                incremental_column=incremental_column,
                batch_size=batch_size,
                stats=stats,
                primary_key=primary_key,
                parallel=parallel,
                num_workers=num_workers
            ),
            name=table_name,
            write_disposition="merge" if incremental_column else "replace",
            primary_key=primary_key,
            columns=dlt_columns  # æ˜¾å¼ä¼ å…¥åˆ—å®šä¹‰
        )


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def format_duration(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´"""
    if seconds < 60:
        return f"{seconds:.2f} ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.2f} åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        return f"{hours:.2f} å°æ—¶"


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def run_sync(
    oracle_conn: str,
    tables: list[dict],
    duckdb_path: str = "oracle_sync.duckdb",
    parallel: bool = False,
    num_workers: int = 4
):
    """
    æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
    
    Args:
        oracle_conn: Oracle è¿æ¥å­—ç¬¦ä¸²
        tables: è¡¨é…ç½®åˆ—è¡¨
        duckdb_path: DuckDB æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        parallel: æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
        num_workers: å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆä»… parallel=True æ—¶ç”Ÿæ•ˆï¼‰
    """
    print("\n" + "=" * 70)
    print("ğŸš€ Oracle â†’ DuckDB å¢é‡åŒæ­¥ä»»åŠ¡")
    print("=" * 70)
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ç›®æ ‡æ•°æ®åº“: {duckdb_path}")
    print(f"ğŸ“‹ åŒæ­¥è¡¨æ•°é‡: {len(tables)}")
    if parallel:
        print(f"ğŸ”§ è¯»å–æ¨¡å¼: å¤šçº¿ç¨‹å¹¶è¡Œ ({num_workers} workers)")
    else:
        print(f"ğŸ”§ è¯»å–æ¨¡å¼: å•çº¿ç¨‹é¡ºåº")
    if oracle_mode == "thick":
        print("ğŸ”§ Oracle æ¨¡å¼: Thick (å…¼å®¹æ‰€æœ‰ç‰ˆæœ¬)")
        client_dir = os.getenv("ORACLE_CLIENT_DIR")
        if client_dir:
            print(f"ğŸ“ Instant Client: {Path(client_dir).name}")
    else:
        print("ğŸ”§ Oracle æ¨¡å¼: Thin (ä»…æ”¯æŒ 12.1+)")
    print()
    
    # è®°å½•æ–‡ä»¶åˆå§‹å¤§å°
    db_file = Path(duckdb_path)
    initial_size = db_file.stat().st_size if db_file.exists() else 0
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {}
    
    # æ˜¾ç¤ºè¡¨é…ç½®
    print("ğŸ“Œ åŒæ­¥é…ç½®:")
    for i, table_config in enumerate(tables, 1):
        table_name = table_config["name"]
        inc_col = table_config.get("incremental_column", "æ— ")
        pk = table_config.get("primary_key", "æ— ")
        print(f"  {i}. {table_name}")
        print(f"     â”œâ”€ å¢é‡åˆ—: {inc_col}")
        print(f"     â””â”€ ä¸»é”®: {pk}")
    print()
    
    # å¼€å§‹åŒæ­¥
    print("â³ å¼€å§‹æ•°æ®åŒæ­¥...")
    start_time = time.time()
    
    try:
        # åˆ›å»º pipelineï¼ˆä¼˜åŒ–é…ç½®ï¼‰
        pipeline = dlt.pipeline(
            pipeline_name="oracle_to_duckdb",
            destination=dlt.destinations.duckdb(duckdb_path),
            dataset_name="oracle_data",
            progress="log"  # æ˜¾ç¤ºå†™å…¥è¿›åº¦
        )
        
        # åŠ è½½æ•°æ®
        source = oracle_source(
            oracle_conn, 
            tables, 
            stats,
            parallel=parallel,
            num_workers=num_workers
        )
        
        print("â³ æ­£åœ¨å†™å…¥ DuckDBï¼ˆå¤§æ•°æ®é‡å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
        load_info = pipeline.run(source)
        
        # è®¡ç®—è€—æ—¶
        end_time = time.time()
        duration = end_time - start_time
        
        # è®¡ç®—æ–‡ä»¶å¤§å°å˜åŒ–
        final_size = db_file.stat().st_size if db_file.exists() else 0
        size_increase = final_size - initial_size
        
        # è®¡ç®—æ€»è¡Œæ•°
        total_rows = sum(s['rows'] for s in stats.values())
        
        # æ˜¾ç¤ºç»“æœ
        print()
        print("=" * 70)
        print("âœ… åŒæ­¥å®Œæˆ!")
        print("=" * 70)
        print()
        
        print("ğŸ“Š åŒæ­¥ç»Ÿè®¡:")
        print(f"  â”œâ”€ æ€»è€—æ—¶: {format_duration(duration)}")
        print(f"  â”œâ”€ æ€»è¡Œæ•°: {total_rows:,} è¡Œ")
        if duration > 0:
            throughput = total_rows / duration
            print(f"  â”œâ”€ ååé‡: {throughput:,.0f} è¡Œ/ç§’")
        print(f"  â”œâ”€ æ•°æ®åº“å¤§å°: {format_size(final_size)}")
        if size_increase > 0:
            print(f"  â””â”€ æ–°å¢æ•°æ®: {format_size(size_increase)}")
        print()
        
        print("ğŸ“‹ å„è¡¨è¯¦æƒ…:")
        for i, (table_name, table_stats) in enumerate(stats.items(), 1):
            mode = table_stats['mode']
            rows = table_stats['rows']
            last_val = table_stats.get('last_value')
            
            print(f"  {i}. {table_name}")
            print(f"     â”œâ”€ æ¨¡å¼: {mode}")
            print(f"     â”œâ”€ è¡Œæ•°: {rows:,}")
            if last_val:
                print(f"     â””â”€ æœ€æ–°å€¼: {last_val}")
            else:
                print(f"     â””â”€ æœ€æ–°å€¼: -")
        
        print()
        print("=" * 70)
        print(f"ğŸ‰ ä»»åŠ¡å®Œæˆäº: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        print()
        
        return load_info
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        print()
        print("=" * 70)
        print("âŒ åŒæ­¥å¤±è´¥!")
        print("=" * 70)
        print(f"â±ï¸  è€—æ—¶: {format_duration(duration)}")
        print(f"â— é”™è¯¯: {str(e)}")
        print("=" * 70)
        print()
        import traceback
        traceback.print_exc()
        raise


# ============================================================================
# é«˜é€ŸåŒæ­¥æ¨¡å¼ï¼ˆPyArrow ç›´æ¥å†™å…¥ + dlt çŠ¶æ€å…³è”ï¼‰
# ============================================================================

def _update_dlt_state(pipeline_name: str, table_name: str, last_value: Any):
    """æ›´æ–° dlt çš„å¢é‡çŠ¶æ€æ–‡ä»¶"""
    import json
    
    # dlt çŠ¶æ€æ–‡ä»¶è·¯å¾„
    # Windows: C:/Users/<user>/.dlt/pipelines/<pipeline_name>/state/
    # Linux/Mac: ~/.dlt/pipelines/<pipeline_name>/state/
    home = Path.home()
    state_dir = home / ".dlt" / "pipelines" / pipeline_name / "state"
    state_file = state_dir / "state.json"
    
    state_dir.mkdir(parents=True, exist_ok=True)
    
    if state_file.exists():
        with open(state_file, 'r') as f:
            state = json.load(f)
    else:
        state = {
            "pipeline_name": pipeline_name,
            "first_run": True,
            "sources": {}
        }
    
    source_key = "oracle_source"
    if "sources" not in state:
        state["sources"] = {}
    if source_key not in state["sources"]:
        state["sources"][source_key] = {"resources": {}}
    if "resources" not in state["sources"][source_key]:
        state["sources"][source_key]["resources"] = {}
    
    state["sources"][source_key]["resources"][table_name] = {
        "last_value": last_value
    }
    state["first_run"] = False
    
    with open(state_file, 'w') as f:
        json.dump(state, f, indent=2, default=str)
    
    logger.info(f"dlt çŠ¶æ€å·²æ›´æ–°: {state_file}")


def _sync_partition_worker(
    oracle_conn: str,
    temp_duckdb_path: str,  # æ”¹ä¸ºç‹¬ç«‹çš„ä¸´æ—¶ DuckDB æ–‡ä»¶è·¯å¾„
    schema: str,
    table_name: str,
    partition_name: str,
    primary_key: str,
    batch_size: int = 50000,
    pyarrow_schema = None  # æ·»åŠ  schema å‚æ•°
) -> dict:
    """
    å•ä¸ªåˆ†åŒºåŒæ­¥å·¥ä½œå‡½æ•°ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼‰
    è¯»å– Oracle åˆ†åŒºæ•°æ®ï¼Œå†™å…¥ç‹¬ç«‹çš„ DuckDB ä¸´æ—¶æ–‡ä»¶
    """
    import duckdb
    import pyarrow as pa
    from sqlalchemy import create_engine
    from decimal import Decimal
    
    start_time = time.time()
    row_count = 0
    max_value = None
    
    full_table = f"{schema}.{table_name}" if schema else table_name
    
    print(f"\n  [åˆ†åŒº {partition_name}] å¼€å§‹è¯»å–...")
    
    # åˆ›å»ºç‹¬ç«‹çš„ Oracle è¿æ¥
    engine = create_engine(
        oracle_conn,
        pool_size=1,
        max_overflow=0,
        pool_pre_ping=True,
        echo=False
    )
    
    # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„ DuckDB ä¸´æ—¶æ–‡ä»¶
    duck_conn = duckdb.connect(temp_duckdb_path)
    
    try:
        with engine.connect() as conn:
            raw_conn = conn.connection
            cursor = raw_conn.cursor()
            cursor.arraysize = 10000
            cursor.prefetchrows = 10000
            
            # è·å–åˆ—å
            cursor.execute(f"SELECT * FROM {full_table} WHERE ROWNUM = 0")
            columns = [desc[0] for desc in cursor.description]
            cursor.close()
            
            # è·å–åˆ†åŒºæ€»è¡Œæ•°ï¼ˆç”¨äºè®¡ç®—è¿›åº¦ï¼‰
            cursor = raw_conn.cursor()
            cursor.execute(f"SELECT COUNT(*) FROM {full_table} PARTITION ({partition_name})")
            total_rows_in_partition = cursor.fetchone()[0]
            cursor.close()
            
            if total_rows_in_partition == 0:
                print(f"  [åˆ†åŒº {partition_name}] å®Œæˆ: 0 è¡Œ, è€—æ—¶ 0.0ç§’    ")
                return {
                    'partition': partition_name,
                    'temp_file': temp_duckdb_path,
                    'rows': 0,
                    'max_value': None,
                    'duration': 0,
                    'success': True
                }
            
            print(f"  [åˆ†åŒº {partition_name}] æ€»è¡Œæ•°: {total_rows_in_partition:,}")
            
            # è¯»å–åˆ†åŒºæ•°æ®
            cursor = raw_conn.cursor()
            cursor.arraysize = 10000
            cursor.prefetchrows = 10000
            
            columns_str = ", ".join(columns)
            query = f"""
                SELECT {columns_str} 
                FROM {full_table} PARTITION ({partition_name})
                ORDER BY {primary_key}
            """
            
            cursor.execute(query)
            
            first_batch = True
            batch_count = 0
            batch_rows = []
            
            # é¢„å¤„ç† Schema ç±»å‹ï¼Œä¼˜åŒ–è½¬æ¢æ€§èƒ½
            float_cols = set()
            int_cols = set()
            if pyarrow_schema:
                for field in pyarrow_schema:
                    if pa.types.is_floating(field.type):
                        float_cols.add(field.name)
                    elif pa.types.is_integer(field.type):
                        int_cols.add(field.name)
            
            while True:
                rows = cursor.fetchmany(batch_size)
                if not rows:
                    break
                
                batch_count += 1
                for row in rows:
                    # å°†è¡Œæ•°æ®è½¬æ¢ä¸ºå­—å…¸
                    row_dict = dict(zip(columns, row))
                    
                    # æ™ºèƒ½ç±»å‹è½¬æ¢ï¼šæ ¹æ® Schema å†³å®šå¦‚ä½•å¤„ç† Decimal
                    for col, val in row_dict.items():
                        if isinstance(val, (Decimal,)):
                            if val is None:
                                continue
                            
                            if col in float_cols:
                                # ç›®æ ‡æ˜¯ floatï¼Œå¼ºåˆ¶è½¬ float
                                row_dict[col] = float(val)
                            elif col in int_cols:
                                # ç›®æ ‡æ˜¯ intï¼Œå¼ºåˆ¶è½¬ int
                                row_dict[col] = int(val)
                            # else: ç›®æ ‡æ˜¯ Decimal/Stringï¼Œä¿æŒåŸæ ·è®© PyArrow å¤„ç†
                                
                    batch_rows.append(row_dict)
                    row_count += 1
                    
                    # è¿½è¸ªæœ€å¤§å€¼
                    if primary_key in row_dict:
                        val = row_dict[primary_key]
                        if max_value is None or val > max_value:
                            max_value = val
                
                # æ¯ batch_size è¡Œå†™å…¥ä¸€æ¬¡
                if len(batch_rows) >= batch_size:
                    arrow_table = pa.Table.from_pylist(batch_rows, schema=pyarrow_schema)
                    
                    if first_batch:
                        duck_conn.execute("DROP TABLE IF EXISTS data")
                        duck_conn.execute("CREATE TABLE data AS SELECT * FROM arrow_table")
                        first_batch = False
                    else:
                        duck_conn.execute("INSERT INTO data SELECT * FROM arrow_table")
                    
                    batch_rows = []
                    
                    elapsed = time.time() - start_time
                    speed = row_count / elapsed if elapsed > 0 else 0
                    # è®¡ç®—è¿›åº¦å’Œé¢„è®¡å‰©ä½™æ—¶é—´
                    progress = (row_count / total_rows_in_partition * 100) if total_rows_in_partition > 0 else 0
                    if speed > 0:
                        remaining_rows = total_rows_in_partition - row_count
                        eta_seconds = remaining_rows / speed
                        eta_str = f"å‰©ä½™: {eta_seconds:.0f}ç§’"
                    else:
                        eta_str = "è®¡ç®—ä¸­..."
                    print(f"  [åˆ†åŒº {partition_name}] {progress:.1f}% | {row_count:,}/{total_rows_in_partition:,} | {speed:,.0f}è¡Œ/ç§’ | {eta_str}", end='\r')
            
            # å¤„ç†å‰©ä½™æ•°æ®
            if batch_rows:
                arrow_table = pa.Table.from_pylist(batch_rows, schema=pyarrow_schema)
                if first_batch:
                    duck_conn.execute("DROP TABLE IF EXISTS data")
                    duck_conn.execute("CREATE TABLE data AS SELECT * FROM arrow_table")
                else:
                    duck_conn.execute("INSERT INTO data SELECT * FROM arrow_table")
            
            cursor.close()
        
        elapsed = time.time() - start_time
        print(f"  [åˆ†åŒº {partition_name}] å®Œæˆ: {row_count:,} è¡Œ, è€—æ—¶ {elapsed:.1f}ç§’    ")
        
        return {
            'partition': partition_name,
            'temp_file': temp_duckdb_path,  # è¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            'rows': row_count,
            'max_value': max_value,
            'duration': elapsed,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"åˆ†åŒº {partition_name} åŒæ­¥å¤±è´¥: {e}")
        return {
            'partition': partition_name,
            'temp_file': temp_duckdb_path,
            'rows': 0,
            'max_value': None,
            'duration': 0,
            'success': False,
            'error': str(e)
        }
    finally:
        duck_conn.close()
        engine.dispose()

def run_fast_sync(
    oracle_conn: str,
    tables: list[dict],
    duckdb_path: str = "oracle_sync.duckdb",
    num_workers: int = 4,
    pipeline_name: str = "oracle_to_duckdb",
    dataset_name: str = "oracle_data",
    use_partition: bool = False
):
    """
    é«˜é€ŸåŒæ­¥ï¼šä½¿ç”¨ PyArrow ç›´æ¥å†™å…¥ DuckDBï¼Œç„¶åæ›´æ–° dlt çŠ¶æ€
    
    é€‚ç”¨äºï¼šé¦–æ¬¡å…¨é‡åŒæ­¥ï¼ˆåƒä¸‡çº§æ•°æ®ï¼‰
    use_partition=True æ—¶ä½¿ç”¨æŒ‰åˆ†åŒºå¹¶è¡Œè¯»å–ï¼ˆæ¨èç”¨äºåˆ†åŒºè¡¨ï¼‰
    åç»­å¢é‡åŒæ­¥ä»å¯ä½¿ç”¨ run_sync()
    """
    import duckdb
    import pyarrow as pa
    
    print("\n" + "=" * 70)
    print("ğŸš€ Oracle â†’ DuckDB é«˜é€ŸåŒæ­¥ï¼ˆPyArrow ç›´æ¥å†™å…¥ï¼‰")
    print("=" * 70)
    print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ç›®æ ‡æ•°æ®åº“: {duckdb_path}")
    print(f"ğŸ“‹ åŒæ­¥è¡¨æ•°é‡: {len(tables)}")
    print(f"ğŸ”§ å¹¶è¡Œçº¿ç¨‹: {num_workers}")
    print(f"ğŸ“‚ è¯»å–æ¨¡å¼: {'æŒ‰åˆ†åŒºå¹¶è¡Œ' if use_partition else 'ID åˆ†ç‰‡å¹¶è¡Œ'}")
    print()
    
    db_file = Path(duckdb_path)
    initial_size = db_file.stat().st_size if db_file.exists() else 0
    start_time = time.time()
    
    duck_conn = duckdb.connect(duckdb_path)
    duck_conn.execute(f"CREATE SCHEMA IF NOT EXISTS {dataset_name}")
    
    results = {}
    
    for table_config in tables:
        table_name = table_config["name"]
        schema = table_config.get("schema")
        primary_key = table_config.get("primary_key", "ID")
        incremental_column = table_config.get("incremental_column")
        batch_size = table_config.get("batch_size", 50000)
        
        print(f"\nğŸ“Š åŒæ­¥è¡¨: {table_name}")
        print("-" * 50)
        
        full_table_name = f"{dataset_name}.{table_name}"
        table_start = time.time()
        
        # åˆ†åŒºæ¨¡å¼ï¼šå¹¶è¡Œè¯»å–å„åˆ†åŒºåˆ°ä¸´æ—¶è¡¨ï¼Œæœ€ååˆå¹¶
        if use_partition:
            # å…ˆåˆ é™¤ç›®æ ‡è¡¨
            duck_conn.execute(f"DROP TABLE IF EXISTS {full_table_name}")
            
            # è·å–åˆ†åŒºåˆ—è¡¨
            reader = OracleParallelReader(
                connection_string=oracle_conn,
                table_name=table_name,
                schema=schema,
                primary_key=primary_key,
                num_workers=num_workers,
                batch_size=batch_size
            )
            
            engine = reader._get_engine()
            
            # è·å–ç»Ÿä¸€çš„ PyArrow schema
            print(f"  ğŸ” è·å–è¡¨ç»“æ„å¹¶ç”Ÿæˆç»Ÿä¸€ Schema...")
            pyarrow_schema, _ = get_table_schema_as_pyarrow(
                engine, 
                table_name, 
                schema
            )
            print(f"  âœ… Schema å·²ç”Ÿæˆ: {[f.name for f in pyarrow_schema]}")
            
            partitions = reader.get_partitions(engine)
            engine.dispose()
            
            print(f"  â”œâ”€ åˆ†åŒºæ•°: {len(partitions)}")
            print(f"  â”œâ”€ å¹¶è¡Œçº¿ç¨‹: {num_workers}")
            print(f"  â”œâ”€ è¯»å–æ¨¡å¼: å¹¶è¡Œåˆ†åŒºå†™å…¥ç‹¬ç«‹ä¸´æ—¶æ–‡ä»¶")
            print(f"  â”œâ”€ åˆ†åŒºåˆ—è¡¨: {partitions[:3]}...{partitions[-2:] if len(partitions) > 5 else ''}")
            print("â³ å¼€å§‹å¹¶è¡ŒåŒæ­¥...")
            
            # å¹¶è¡Œå¤„ç†åˆ†åŒº
            partition_results = []
            
            # è·å– DuckDB æ–‡ä»¶æ‰€åœ¨ç›®å½•
            import os
            duckdb_dir = os.path.dirname(os.path.abspath(duckdb_path))
            
            with ThreadPoolExecutor(max_workers=num_workers) as executor:
                futures = []
                for idx, partition in enumerate(partitions):
                    # æ¯ä¸ªåˆ†åŒºä½¿ç”¨ç‹¬ç«‹çš„ä¸´æ—¶ DuckDB æ–‡ä»¶
                    temp_file = os.path.join(duckdb_dir, f"temp_partition_{idx}.duckdb")
                    
                    future = executor.submit(
                        _sync_partition_worker,
                        oracle_conn,
                        temp_file,  # ç‹¬ç«‹çš„ä¸´æ—¶æ–‡ä»¶
                        schema,
                        table_name,
                        partition,
                        primary_key,
                        batch_size,
                        pyarrow_schema  # ä¼ é€’ç»Ÿä¸€ schema
                    )
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰åˆ†åŒºå®Œæˆ
                for future in futures:
                    result = future.result()
                    partition_results.append(result)
            
            # ç»Ÿè®¡ç»“æœ
            total_rows = sum(r['rows'] for r in partition_results if r['success'])
            max_value = None
            for r in partition_results:
                if r['success'] and r['max_value'] is not None:
                    if max_value is None or r['max_value'] > max_value:
                        max_value = r['max_value']
            
            # åªæ”¶é›†æˆåŠŸä¸”æœ‰æ•°æ®çš„ä¸´æ—¶æ–‡ä»¶
            valid_temp_files = []
            for r in partition_results:
                if r['success'] and r['rows'] > 0:
                    valid_temp_files.append(r['temp_file'])
            
            print(f"\n  ğŸ“¦ åˆå¹¶ {len(valid_temp_files)} ä¸ªæœ‰æ•ˆä¸´æ—¶æ–‡ä»¶...")
            
            if valid_temp_files:
                # å…ˆ ATTACH æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                for idx, temp_file in enumerate(valid_temp_files):
                    db_alias = f"temp_db_{idx}"
                    try:
                        duck_conn.execute(f"ATTACH '{temp_file}' AS {db_alias} (READ_ONLY)")
                    except Exception as e:
                        logger.warning(f"ATTACH ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
                
                # ä½¿ç”¨ UNION ALL BY NAME ä¸€æ¬¡æ€§åˆå¹¶ï¼ˆè‡ªåŠ¨å¤„ç†ç±»å‹å·®å¼‚ï¼‰
                union_parts = [f"SELECT * FROM temp_db_{idx}.data" for idx in range(len(valid_temp_files))]
                union_query = " UNION ALL BY NAME ".join(union_parts)
                
                try:
                    duck_conn.execute(f"CREATE TABLE {full_table_name} AS {union_query}")
                    print(f"  â”œâ”€ åˆå¹¶å®Œæˆ")
                except Exception as e:
                    logger.error(f"åˆå¹¶å¤±è´¥: {e}")
                    # å›é€€åˆ°é€ä¸ªåˆå¹¶
                    print(f"  â”œâ”€ UNION ALL å¤±è´¥ï¼Œå°è¯•é€ä¸ªåˆå¹¶...")
                    first_file = True
                    for idx in range(len(valid_temp_files)):
                        try:
                            if first_file:
                                duck_conn.execute(f"CREATE TABLE {full_table_name} AS SELECT * FROM temp_db_{idx}.data")
                                first_file = False
                            else:
                                # ä½¿ç”¨ INSERT OR IGNORE å¿½ç•¥ç±»å‹é”™è¯¯
                                duck_conn.execute(f"INSERT INTO {full_table_name} SELECT * FROM temp_db_{idx}.data")
                        except Exception as ex:
                            logger.warning(f"åˆå¹¶ temp_db_{idx} å¤±è´¥: {ex}")
                
                # DETACH æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
                for idx in range(len(valid_temp_files)):
                    try:
                        duck_conn.execute(f"DETACH temp_db_{idx}")
                    except:
                        pass
            else:
                print(f"  â”œâ”€ æ²¡æœ‰æœ‰æ•ˆçš„ä¸´æ—¶æ–‡ä»¶")
                row_count = 0
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            for r in partition_results:
                try:
                    temp_file = r.get('temp_file')
                    if temp_file and os.path.exists(temp_file):
                        os.remove(temp_file)
                        # åŒæ—¶åˆ é™¤ WAL æ–‡ä»¶
                        wal_file = temp_file + ".wal"
                        if os.path.exists(wal_file):
                            os.remove(wal_file)
                except Exception:
                    pass
            print(f"  â”œâ”€ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
            
            row_count = total_rows
            
        else:
            # éåˆ†åŒºæ¨¡å¼ï¼šä½¿ç”¨åŸæœ‰çš„ ID åˆ†ç‰‡å¹¶è¡Œè¯»å–
            reader = OracleParallelReader(
                connection_string=oracle_conn,
                table_name=table_name,
                schema=schema,
                primary_key=primary_key,
                num_workers=num_workers,
                batch_size=batch_size
            )
            
            # è·å–ç»Ÿä¸€çš„ PyArrow schema
            engine = reader._get_engine()
            print(f"  ğŸ” è·å–è¡¨ç»“æ„å¹¶ç”Ÿæˆç»Ÿä¸€ Schema...")
            pyarrow_schema, _ = get_table_schema_as_pyarrow(engine, table_name, schema)
            engine.dispose()
            print(f"  âœ… Schema å·²ç”Ÿæˆ")
            
            row_count = 0
            max_value = None
            batch_rows = []
            batch_num = 0
            first_batch = True
            
            duck_conn.execute(f"DROP TABLE IF EXISTS {full_table_name}")
            
            print("â³ å¼€å§‹è¯»å–å’Œå†™å…¥...")
            
            # é¢„å¤„ç† Schema ç±»å‹ï¼Œä¼˜åŒ–è½¬æ¢æ€§èƒ½
            float_cols = set()
            int_cols = set()
            if pyarrow_schema:
                for field in pyarrow_schema:
                    if pa.types.is_floating(field.type):
                        float_cols.add(field.name)
                    elif pa.types.is_integer(field.type):
                        int_cols.add(field.name)
            
            for row in reader.parallel_read(None):
                batch_rows.append(row)
                
                if incremental_column and incremental_column in row:
                    val = row[incremental_column]
                    if max_value is None or val > max_value:
                        max_value = val
                
                # æ™ºèƒ½ç±»å‹è½¬æ¢ï¼šæ ¹æ® Schema å†³å®šå¦‚ä½•å¤„ç† Decimal
                for col, val in row.items():
                    if isinstance(val, (Decimal,)):
                        if val is None:
                            continue
                        
                        if col in float_cols:
                            row[col] = float(val)
                        elif col in int_cols:
                            row[col] = int(val)
                        # else: ç›®æ ‡æ˜¯ Decimal/Stringï¼Œä¿æŒåŸæ ·è®© PyArrow å¤„ç†
                
                if len(batch_rows) >= batch_size:
                    batch_num += 1
                    arrow_table = pa.Table.from_pylist(batch_rows, schema=pyarrow_schema)
                    
                    if first_batch:
                        duck_conn.execute(f"CREATE TABLE {full_table_name} AS SELECT * FROM arrow_table")
                        first_batch = False
                    else:
                        duck_conn.execute(f"INSERT INTO {full_table_name} SELECT * FROM arrow_table")
                    
                    row_count += len(batch_rows)
                    elapsed = time.time() - table_start
                    speed = row_count / elapsed if elapsed > 0 else 0
                    print(f"  â”œâ”€ å·²å†™å…¥: {row_count:,} è¡Œ | é€Ÿåº¦: {speed:,.0f} è¡Œ/ç§’ | æ‰¹æ¬¡ #{batch_num}", end='\r')
                    
                    batch_rows = []
            
            if batch_rows:
                arrow_table = pa.Table.from_pylist(batch_rows, schema=pyarrow_schema)
                if first_batch:
                    duck_conn.execute(f"CREATE TABLE {full_table_name} AS SELECT * FROM arrow_table")
                else:
                    duck_conn.execute(f"INSERT INTO {full_table_name} SELECT * FROM arrow_table")
                row_count += len(batch_rows)
        
        table_elapsed = time.time() - table_start
        table_speed = row_count / table_elapsed if table_elapsed > 0 else 0
        
        print(f"\n  â””â”€ è¡¨ {table_name} å®Œæˆ: {row_count:,} è¡Œ | è€—æ—¶: {table_elapsed:.1f}ç§’ | é€Ÿåº¦: {table_speed:,.0f} è¡Œ/ç§’")
        
        results[table_name] = {
            'rows': row_count,
            'max_value': max_value,
            'duration': table_elapsed
        }
        
        if incremental_column and max_value is not None:
            _update_dlt_state(
                pipeline_name=pipeline_name,
                table_name=table_name,
                last_value=max_value
            )
            print(f"  âœ… dlt çŠ¶æ€å·²æ›´æ–°: {incremental_column} = {max_value}")
    
    duck_conn.close()
    
    end_time = time.time()
    duration = end_time - start_time
    total_rows = sum(r['rows'] for r in results.values())
    
    final_size = db_file.stat().st_size if db_file.exists() else 0
    size_increase = final_size - initial_size
    
    print()
    print("=" * 70)
    print("âœ… é«˜é€ŸåŒæ­¥å®Œæˆ!")
    print("=" * 70)
    print(f"  â”œâ”€ æ€»è€—æ—¶: {format_duration(duration)}")
    print(f"  â”œâ”€ æ€»è¡Œæ•°: {total_rows:,} è¡Œ")
    if duration > 0:
        print(f"  â”œâ”€ å¹³å‡é€Ÿåº¦: {total_rows/duration:,.0f} è¡Œ/ç§’")
    print(f"  â”œâ”€ æ•°æ®åº“å¤§å°: {format_size(final_size)}")
    if size_increase > 0:
        print(f"  â””â”€ æ–°å¢æ•°æ®: {format_size(size_increase)}")
    print()
    print("ğŸ’¡ åç»­å¢é‡åŒæ­¥è¯·ä½¿ç”¨: run_sync(...)")
    print("=" * 70)
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Oracle â†’ DuckDB æ•°æ®åŒæ­¥å·¥å…·")
    parser.add_argument("--fast", action="store_true", 
                        help="ä½¿ç”¨é«˜é€Ÿæ¨¡å¼ï¼ˆPyArrow ç›´æ¥å†™å…¥ï¼‰ï¼Œé€‚åˆé¦–æ¬¡å…¨é‡åŒæ­¥")
    parser.add_argument("--partition", action="store_true",
                        help="æŒ‰åˆ†åŒºå¹¶è¡Œè¯»å–ï¼ˆæ¨èç”¨äºåˆ†åŒºè¡¨ï¼Œå¦‚ IM_ZJ_LSï¼‰")
    parser.add_argument("--workers", type=int, default=4,
                        help="å¹¶è¡Œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰")
    parser.add_argument("--batch-size", type=int, default=50000,
                        help="æ¯æ‰¹è¯»å–è¡Œæ•°ï¼ˆé»˜è®¤: 50000ï¼‰")
    parser.add_argument("--db", type=str, default="oracle_sync.duckdb",
                        help="DuckDB æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤: oracle_sync.duckdbï¼‰")
    
    # æ™ºèƒ½åŒæ­¥å‚æ•°
    parser.add_argument("--smart", action="store_true",
                        help="å¯ç”¨ LLM æ™ºèƒ½åŒæ­¥æ¨¡å¼ï¼ˆè‡ªåŠ¨åˆ†æè¡¨å¹¶æ¨èç­–ç•¥ï¼‰")
    parser.add_argument("--auto", action="store_true",
                        help="å…¨è‡ªåŠ¨æ¨¡å¼ï¼šè·³è¿‡å®¡æ‰¹ç¡®è®¤ï¼ˆä»… --smart æ¨¡å¼æœ‰æ•ˆï¼‰")
    parser.add_argument("--schema", type=str, default=None,
                        help="Oracle schemaï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡ ORACLE_SCHEMA è¯»å–ï¼‰")
    parser.add_argument("--tables", type=str, default="*",
                        help="è¦åŒæ­¥çš„è¡¨ï¼Œé€—å·åˆ†éš”æˆ– * è¡¨ç¤ºå…¨éƒ¨ï¼ˆé»˜è®¤: *ï¼‰")
    
    args = parser.parse_args()
    
    # æ™ºèƒ½åŒæ­¥æ¨¡å¼
    if args.smart:
        from hpf_platform.etl.smart_sync import smart_sync
        from hpf_platform.etl.config import ORACLE_CONFIG
        
        # è§£æè¡¨å‚æ•°
        if args.tables == "*":
            tables = ["*"]
        else:
            tables = [t.strip() for t in args.tables.split(",")]
        
        schema = args.schema or ORACLE_CONFIG.get("default_schema")
        
        result = smart_sync(
            tables=tables,
            schema=schema,
            approval_mode=not args.auto
        )
        
        if result.get("status") == "success":
            print("\nâœ… æ™ºèƒ½åŒæ­¥æˆåŠŸå®Œæˆ")
        elif result.get("status") == "partial":
            print("\nâš ï¸  æ™ºèƒ½åŒæ­¥éƒ¨åˆ†å®Œæˆï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        else:
            print(f"\nâŒ æ™ºèƒ½åŒæ­¥å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
    
    else:
        # ä¼ ç»ŸåŒæ­¥æ¨¡å¼
        # ä» dlt secrets è¯»å–é…ç½®
        oracle_connection = dlt.secrets["sources.oracle_db.credentials"]
        
        # é…ç½®è¦åŒæ­¥çš„è¡¨
        tables_to_sync = [
            {
                "name": "IM_ZJ_LS",
                "incremental_column": "ID",
                "primary_key": "ID",
                "schema": "SHINEYUE40_BZBGJJYW_CS",  # æ·»åŠ  schema
                "batch_size": args.batch_size
            }
        ]
        
        # æ ¹æ®å‚æ•°é€‰æ‹©åŒæ­¥æ¨¡å¼
        if args.fast:
            # é«˜é€Ÿæ¨¡å¼ï¼šPyArrow ç›´æ¥å†™å…¥
            run_fast_sync(
                oracle_conn=oracle_connection,
                tables=tables_to_sync,
                duckdb_path=args.db,
                num_workers=args.workers,
                use_partition=args.partition  # ä¼ é€’åˆ†åŒºæ¨¡å¼å‚æ•°
            )
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šdlt pipeline
            run_sync(
                oracle_conn=oracle_connection,
                tables=tables_to_sync,
                duckdb_path=args.db,
                parallel=True,
                num_workers=args.workers
            )

