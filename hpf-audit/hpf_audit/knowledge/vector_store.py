"""
å‘é‡å­˜å‚¨ç®¡ç†å™¨ (åŸºäºLangChain + FAISS)
ä½¿ç”¨API Embeddingï¼Œä¸ä¸‹è½½æœ¬åœ°æ¨¡å‹
"""
from typing import List, Dict, Optional
from pathlib import Path
import pickle
import numpy as np

try:
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    from langchain_core.embeddings import Embeddings
    LANGCHAIN_AVAILABLE = True
except ImportError:
    pass
    LANGCHAIN_AVAILABLE = False
    print("Warning: LangChain not available, vector store disabled")


class APIEmbeddings(Embeddings):
    """ä½¿ç”¨hpf_commonçš„API Embedding"""
    
    def __init__(self):
        from hpf_common.embedding import EmbeddingClient
        self.client = EmbeddingClient()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥å¤šä¸ªæ–‡æ¡£"""
        # ç›´æ¥ä¼ é€’åˆ—è¡¨ç»™client (clientå·²å¤„ç†æ‰¹é‡)
        # client.embed è¿”å› List[List[float]]
        return self.client.embed(texts)
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        # å–ç¬¬ä¸€æ¡ç»“æœ
        result = self.client.embed(text)
        return result[0] if result else []


class VectorStoreManager:
    """ç»Ÿä¸€çš„å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self, 
        index_path: str = "data/faiss_index"
    ):
        if not LANGCHAIN_AVAILABLE:
            raise ImportError("LangChain is required. Run: pip install langchain langchain-community faiss-cpu")
        
        self.index_path = Path(index_path)
        self.index_path.mkdir(parents=True, exist_ok=True)
        
        # âœ… ä½¿ç”¨API Embedding (ä¸ä¸‹è½½æ¨¡å‹)
        print(f"ğŸ“¡ ä½¿ç”¨API Embedding (ä¸ä¸‹è½½æœ¬åœ°æ¨¡å‹)")
        self.embeddings = APIEmbeddings()
        
        # åŠ è½½æˆ–åˆ›å»ºå‘é‡å­˜å‚¨
        self.vectorstore = self._load_or_create()
        print(f"âœ… å‘é‡å­˜å‚¨å·²å°±ç»ª: {self.index_path}")
    
    def _load_or_create(self) -> FAISS:
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•"""
        index_file = self.index_path / "index.faiss"
        
        if index_file.exists():
            try:
                return FAISS.load_local(
                    str(self.index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç´¢å¼•å¤±è´¥: {e}, åˆ›å»ºæ–°ç´¢å¼•")
        
        # åˆ›å»ºç©ºç´¢å¼•
        return FAISS.from_documents(
            [Document(page_content="åˆå§‹åŒ–æ–‡æ¡£", metadata={"type": "init"})],
            self.embeddings
        )
    
    def add_skills(self, skills: List[Dict]):
        """
        æ·»åŠ Skillsåˆ°å‘é‡åº“
        
        Args:
            skills: [
                {
                    "skill_id": "withdrawal_audit",
                    "name": "æå–å®¡è®¡",
                    "description": "æ£€æŸ¥æå–ä¸šåŠ¡å¼‚å¸¸...",
                }
            ]
        """
        documents = []
        for skill in skills:
            # æ„å»ºæ£€ç´¢æ–‡æœ¬ (ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ content)
            if "content" in skill and skill["content"]:
                content = skill["content"]
            else:
                content = f"""
æŠ€èƒ½åç§°: {skill['name']}
æŠ€èƒ½ID: {skill['skill_id']}
åŠŸèƒ½æè¿°: {skill['description']}
                """.strip()
            
            metadata = {
                "skill_id": skill["skill_id"],
                "name": skill["name"],
                "type": "skill"
            }
            # åˆå¹¶è‡ªå®šä¹‰å…ƒæ•°æ®
            if "metadata" in skill:
                metadata.update(skill["metadata"])

            doc = Document(
                page_content=content,
                metadata=metadata
            )
            documents.append(doc)
        
        ids = [skill["skill_id"] for skill in skills]
        
        # æ·»åŠ åˆ°å‘é‡åº“ (ä½¿ç”¨skill_idä½œä¸ºæ–‡æ¡£ID)
        self.vectorstore.add_documents(documents, ids=ids)
        self.save()
        print(f"âœ… å·²ç´¢å¼• {len(skills)} ä¸ªSkills")

    def add_knowledge(self, items: List[Dict]):
        """
        æ·»åŠ é€šç”¨çŸ¥è¯†åˆ°å‘é‡åº“
        
        Args:
            items: [
                {
                    "id": 1,
                    "title": "...",
                    "content": "...",
                    "category": "regulation",
                    "tags": "a,b"
                }
            ]
        """
        if not items: return
        
        documents = []
        ids = []
        for item in items:
            content = f"{item['title']}\n{item['content']}"
            doc_id = f"kb_{item['id']}"
            
            doc = Document(
                page_content=content,
                metadata={
                    "id": item["id"],
                    "title": item["title"],
                    "category": item["category"],
                    "tags": item.get("tags", ""),
                    "type": "knowledge"
                }
            )
            documents.append(doc)
            ids.append(doc_id)
            
        self.vectorstore.add_documents(documents, ids=ids)
        self.save()
        print(f"âœ… å·²ç´¢å¼• {len(items)} æ¡çŸ¥è¯†")

    def delete_document(self, doc_id: str):
        """ä»å‘é‡åº“åˆ é™¤æ–‡æ¡£ (é€šç”¨)"""
        try:
            self.vectorstore.delete([doc_id])
            self.save()
            print(f"âœ… å·²ä»å‘é‡åº“åˆ é™¤: {doc_id}")
            return True
        except Exception as e:
            print(f"âš ï¸ ä»å‘é‡åº“åˆ é™¤å¤±è´¥: {e}")
            return False

    def delete_skill(self, skill_id: str):
        """å…¼å®¹æ—§æ¥å£: åˆ é™¤ Skill"""
        return self.delete_document(skill_id)

    def search(
        self, 
        query: str, 
        top_k: int = 3,
        filter_dict: Optional[Dict] = None
    ) -> List[Dict]:
        """
        é€šç”¨è¯­ä¹‰æœç´¢
        """
        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query,
            k=top_k,
            filter=filter_dict
        )
        
        results = []
        for doc, score in docs_with_scores:
            if doc.metadata.get("type") == "init": continue
                
            results.append({
                "content": doc.page_content,
                "score": float(1 - score),
                "metadata": doc.metadata
            })
        return results
    
    def search_skills(
        self, 
        query: str, 
        top_k: int = 3,
        filter_dict: Optional[Dict] = None
    ) -> List[Dict]:
        """
        è¯­ä¹‰æœç´¢Skills
        
        Returns:
            [
                {
                    "skill_id": "...",
                    "name": "...",
                    "score": 0.85,
                    "content": "..."
                }
            ]
        """
        # ä½¿ç”¨LangChainçš„ç›¸ä¼¼åº¦æœç´¢
        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query,
            k=top_k,
            filter=filter_dict
        )
        
        results = []
        for doc, score in docs_with_scores:
            # è¿‡æ»¤æ‰åˆå§‹åŒ–æ–‡æ¡£
            if doc.metadata.get("type") == "init":
                continue
                
            results.append({
                "skill_id": doc.metadata.get("skill_id"),
                "name": doc.metadata.get("name"),
                "score": float(1 - score),  # FAISSè¿”å›çš„æ˜¯è·ç¦»ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
                "content": doc.page_content
            })
        
        return results
    
    def save(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜"""
        self.vectorstore.save_local(str(self.index_path))
    
    def get_stats(self) -> Dict:
        """è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": self.vectorstore.index.ntotal if hasattr(self.vectorstore, 'index') else 0,
            "index_path": str(self.index_path),
            "embedding_type": "API (NVIDIA/Cerebras)"
        }
